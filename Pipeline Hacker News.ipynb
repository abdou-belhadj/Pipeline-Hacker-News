{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We will build a robust data pipeline that schedules our tasks in the correct order and we will apply it to a real world data !! We will filter, clean, aggregate and summarize the data in a sequence of tasks.\n",
    "\n",
    "The data we will use comes from a Hacker News (HN) API that returns JSON DATA of the top stories in 2014, it looks like the one used in [this project](https://github.com/abdouch96/Exploring-Hackers-News-Posts-/blob/main/Exploring%20Hackers%20News%20Posts%20.ipynb).\n",
    "\n",
    "If you're unfamiliar with Hacker News, it's a link aggregator website that users vote up stories that are interesting to the community. It is similar to [Reddit](https://www.reddit.com/), but the community only revolves around on computer science and entrepreneurship posts.\n",
    "\n",
    "The JSON file contains a single key stories, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
    "\n",
    "  -  title: The headline of the post.\n",
    "  -  created_at: A timestamp of the story's creation time.\n",
    "  -  url: The URL of the story link.\n",
    "  -  objectID: The ID of the story.\n",
    "  -  points: The number of upvotes the story had.\n",
    "\n",
    "\n",
    "Using this dataset, we will run a sequence of basic natural language processing tasks using our Pipeline class. The goal will be to find the top 100 keywords of Hacker News posts in 2014. Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import deque\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import json\n",
    "import io\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAG:\n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "\n",
    "    def in_degrees(self):\n",
    "        in_degrees = {}\n",
    "        for node in self.graph:\n",
    "            if node not in in_degrees:\n",
    "                in_degrees[node] = 0\n",
    "            for pointed in self.graph[node]:\n",
    "                if pointed not in in_degrees:\n",
    "                    in_degrees[pointed] = 0\n",
    "                in_degrees[pointed] += 1\n",
    "        return in_degrees\n",
    "\n",
    "    def sort(self):\n",
    "        in_degrees = self.in_degrees()\n",
    "        to_visit = deque()\n",
    "        for node in self.graph:\n",
    "            if in_degrees[node] == 0:\n",
    "                to_visit.append(node)\n",
    "\n",
    "        searched = []\n",
    "        while to_visit:\n",
    "            node = to_visit.popleft()\n",
    "            for pointer in self.graph[node]:\n",
    "                in_degrees[pointer] -= 1\n",
    "                if in_degrees[pointer] == 0:\n",
    "                    to_visit.append(pointer)\n",
    "            searched.append(node)\n",
    "        return searched\n",
    "\n",
    "    def add(self, node, to=None):\n",
    "        if node not in self.graph:\n",
    "            self.graph[node] = []\n",
    "        if to:\n",
    "            if to not in self.graph:\n",
    "                self.graph[to] = []\n",
    "            self.graph[node].append(to)\n",
    "        if len(self.sort()) != len(self.graph):\n",
    "            raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = DAG()\n",
    "\n",
    "    def task(self, depends_on=None):\n",
    "        def inner(f):\n",
    "            self.tasks.add(f)\n",
    "            if depends_on:\n",
    "                self.tasks.add(depends_on, f)\n",
    "            return f\n",
    "        return inner\n",
    "\n",
    "    def run(self):\n",
    "        scheduled = self.tasks.sort()\n",
    "        completed = {}\n",
    "\n",
    "        for task in scheduled:\n",
    "            for node, values in self.tasks.graph.items():\n",
    "                if task in values:\n",
    "                    completed[task] = task(completed[node])\n",
    "            if task not in completed:\n",
    "                completed[task] = task()\n",
    "        return completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_csv(lines, header=None, file=None):\n",
    "    if header:\n",
    "        lines = itertools.chain([header], lines)\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hn', 211), ('show', 193), ('new', 187), ('google', 179), ('bitcoin', 104), ('open', 94), ('web', 92), ('programming', 91), ('data', 87), ('us', 86), ('video', 80), ('python', 77), ('facebook', 73), ('code', 73), ('using', 72), ('released', 72), ('c', 68), ('javascript', 66), ('internet', 65), ('free', 65), ('game', 65), ('source', 65), ('first', 63), ('go', 62), ('microsoft', 60), ('linux', 60), ('one', 60), ('app', 59), ('pdf', 56), ('work', 55), ('language', 55), ('apple', 53), ('software', 53), ('startup', 53), ('use', 51), ('make', 51), ('world', 50), ('nsa', 50), ('yc', 50), ('time', 49), ('security', 49), ('get', 46), ('github', 46), ('system', 45), ('x', 45), ('windows', 45), ('way', 42), ('like', 42), ('heartbleed', 42), ('project', 41), ('computer', 41), ('ios', 40), ('dont', 39), ('git', 38), ('users', 38), ('back', 38), ('twitter', 38), ('design', 38), ('day', 38), ('developer', 37), ('os', 37), ('ceo', 37), ('vs', 37), ('big', 37), ('life', 37), ('android', 35), ('simple', 35), ('online', 35), ('made', 34), ('years', 34), ('amazon', 34), ('court', 34), ('mozilla', 34), ('year', 34), ('firefox', 33), ('guide', 33), ('learning', 33), ('mt', 33), ('api', 33), ('says', 33), ('apps', 33), ('browser', 33), ('server', 32), ('fast', 32), ('gox', 32), ('problem', 32), ('engine', 32), ('site', 32), ('introducing', 31), ('support', 30), ('stop', 30), ('built', 30), ('better', 30), ('million', 30), ('html', 30), ('people', 30), ('text', 30), ('tech', 29), ('development', 29), ('k', 29)]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('OneDrive\\Documents\\my_datasets\\hn_stories_2014.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "    return stories\n",
    "\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['points'] > 50 and story['num_comments'] > 1 and not story['title'].startswith('Ask HN')\n",
    "\n",
    "    return (story for story in stories if is_popular(story))\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append(\n",
    "            (story['objectID'], \n",
    "             datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"), \n",
    "             story['url'], \n",
    "             story['points'], \n",
    "             story['title'])\n",
    "        )\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    \n",
    "    return (line[idx] for line in reader)\n",
    "\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_title(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(w for w in title if w not in string.punctuation)\n",
    "        title = re.sub(r'[^a-z]', ' ', title)\n",
    "        yield title\n",
    "        \n",
    "@pipeline.task(depends_on=clean_title)\n",
    "def build_keyword_dictionary(titles):\n",
    "    stopword = stopwords.words('english')\n",
    "    word_freq = {}\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stopword:\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                word_freq[word] += 1\n",
    "    return word_freq\n",
    "\n",
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def top_keywords(word_freq):\n",
    "    freq_tuple = [ (word, word_freq[word]) for word in sorted(word_freq, key=word_freq.get, reverse=True) ]\n",
    "    return freq_tuple[:100]\n",
    "\n",
    "top_100 = pipeline.run()\n",
    "print(top_100[top_keywords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<function __main__.file_to_json()>: [<function __main__.filter_stories(stories)>],\n",
       " <function __main__.filter_stories(stories)>: [<function __main__.json_to_csv(stories)>],\n",
       " <function __main__.json_to_csv(stories)>: [<function __main__.extract_titles(csv_file)>],\n",
       " <function __main__.extract_titles(csv_file)>: [<function __main__.clean_title(titles)>],\n",
       " <function __main__.clean_title(titles)>: [<function __main__.build_keyword_dictionary(titles)>],\n",
       " <function __main__.build_keyword_dictionary(titles)>: [<function __main__.top_keywords(word_freq)>],\n",
       " <function __main__.top_keywords(word_freq)>: []}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = pipeline.tasks.graph\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file_to_json', 'filter_stories', 'json_to_csv', 'extract_titles', 'clean_title', 'build_keyword_dictionary', 'top_keywords']\n"
     ]
    }
   ],
   "source": [
    "dependencies = [func.__name__ for func in pipeline.tasks.sort()]\n",
    "print(dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
